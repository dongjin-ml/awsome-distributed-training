# MPT-7B benchmark
Mosaic Pretrained Transformers (MPT) are GPT-style models with some special features -- Flash Attention for efficiency, ALiBi for context length extrapolation, and stability improvements to mitigate loss spikes. 

Directory structure is as follows:

## Background
Customer NTT 

## Experimental settings

base image: mosaicml/pytorch:2.0.1_cu118-python3.10-ubuntu20.04
LLM-Foundry commit hash: 62e2feac728605cff9306a1e5a51a2903a87ac55
dataset: c4

### Single-node bench

### Multi-node bench
Add pcluster settings here


## Result


## Discussion

